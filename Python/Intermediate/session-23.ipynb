{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "355b5f03-d2a5-4aa3-a77c-776ccfcb35d5",
   "metadata": {},
   "source": [
    "# Session 23 üêç"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45efc9ec-fb9c-458c-b1de-d961145e8449",
   "metadata": {},
   "source": [
    "‚òÄÔ∏è‚òÄÔ∏è‚òÄÔ∏è‚òÄÔ∏è‚òÄÔ∏è‚òÄÔ∏è‚òÄÔ∏è‚òÄÔ∏è‚òÄÔ∏è‚òÄÔ∏è‚òÄÔ∏è‚òÄÔ∏è‚òÄÔ∏è‚òÄÔ∏è‚òÄÔ∏è‚òÄÔ∏è‚òÄÔ∏è‚òÄÔ∏è‚òÄÔ∏è‚òÄÔ∏è‚òÄÔ∏è‚òÄÔ∏è‚òÄÔ∏è‚òÄÔ∏è‚òÄÔ∏è‚òÄÔ∏è‚òÄÔ∏è‚òÄÔ∏è‚òÄÔ∏è‚òÄÔ∏è‚òÄÔ∏è‚òÄÔ∏è‚òÄÔ∏è‚òÄÔ∏è‚òÄÔ∏è‚òÄÔ∏è‚òÄÔ∏è‚òÄÔ∏è‚òÄÔ∏è‚òÄÔ∏è‚òÄÔ∏è‚òÄÔ∏è‚òÄÔ∏è‚òÄÔ∏è‚òÄÔ∏è"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c7a6ce5-4cd2-45cd-a3b8-9ac277170654",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f47542a8-416d-4a3f-8c9d-99781dc53bd8",
   "metadata": {},
   "source": [
    "# 183. Scrapy\n",
    "`Scrapy` is a powerful and popular web scraping framework for Python. It provides a complete toolkit for extracting data from websites, processing it, and storing it in your preferred format. Let's explore Scrapy in detail."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cccc2432-7d24-4f29-b7a2-300e65304cd8",
   "metadata": {},
   "source": [
    "# 184. Core Components"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a35384c2-ea49-4613-b074-ef3b571f756e",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e4dbbf4-88e4-4a78-b9a0-fd722e1aa1c0",
   "metadata": {},
   "source": [
    "## 184-1. Spiders\n",
    "Spiders are classes that define how to scrape a website. Here's a basic spider example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5891a8e6-0f4e-415c-93b9-a89c6ff41cd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import scrapy\n",
    "\n",
    "class MySpider(scrapy.Spider):\n",
    "    name = 'myspider'\n",
    "    start_urls = ['http://example.com']\n",
    "\n",
    "    def parse(self, response):\n",
    "        # Extract data from the page\n",
    "        title = response.css('h1::text').get()\n",
    "        yield {'title': title}\n",
    "\n",
    "        # Follow links to other pages\n",
    "        for link in response.css('a::attr(href)').getall():\n",
    "            yield response.follow(link, callback=self.parse)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4bb5edf-3034-4d79-8edf-8f0740ea0e27",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb5cc52c-9d10-47ad-baeb-0900ab6358ff",
   "metadata": {},
   "source": [
    "## 184-2. Items\n",
    "Items define the structure of the scraped data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29210a7b-c9d9-4522-abc0-243a6ad02c47",
   "metadata": {},
   "outputs": [],
   "source": [
    "import scrapy\n",
    "\n",
    "class ProductItem(scrapy.Item):\n",
    "    name = scrapy.Field()\n",
    "    price = scrapy.Field()\n",
    "    description = scrapy.Field()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d2de8aa-11c8-4023-a19b-77faa03c14af",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7528c824-0f13-4c5d-aa16-25b5deb65b5f",
   "metadata": {},
   "source": [
    "## 184-3. Item Pipelines\n",
    "Pipelines process the scraped items:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e390a33-4241-4879-8d1c-e61489274e70",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PriceConversionPipeline:\n",
    "    def process_item(self, item, spider):\n",
    "        if 'price' in item:\n",
    "            item['price'] = float(item['price'].replace('$', ''))\n",
    "        return item\n",
    "\n",
    "class JsonWriterPipeline:\n",
    "    def open_spider(self, spider):\n",
    "        self.file = open('items.json', 'w')\n",
    "\n",
    "    def close_spider(self, spider):\n",
    "        self.file.close()\n",
    "\n",
    "    def process_item(self, item, spider):\n",
    "        line = json.dumps(dict(item)) + \"\\n\"\n",
    "        self.file.write(line)\n",
    "        return item"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89d79214-4d24-4f11-bff3-2a5c092c85e7",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d78187cc-be37-4780-af75-09adb0339955",
   "metadata": {},
   "source": [
    "## 184-4. Middleware\n",
    "Middleware can process requests and responses:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49e05f1b-864d-4878-8daf-c7ca191680b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomUserAgentMiddleware:\n",
    "    def process_request(self, request, spider):\n",
    "        request.headers['User-Agent'] = 'My Custom User Agent'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5d2ee3a-efa5-4cc4-a34a-77e00ce9a9a4",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6054fd00-b5cb-4760-84ea-8f2798c54d08",
   "metadata": {},
   "source": [
    "# 185. Project Structure\n",
    "A typical Scrapy project looks like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d83dc60-4902-4e04-bac9-c85ffd059dc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "myproject/\n",
    "    scrapy.cfg            # Deployment configuration\n",
    "    myproject/           # Project's Python module\n",
    "        __init__.py\n",
    "        items.py         # Item definitions\n",
    "        middlewares.py   # Project middlewares\n",
    "        pipelines.py      # Project pipelines\n",
    "        settings.py       # Project settings\n",
    "        spiders/         # Spiders directory\n",
    "            __init__.py\n",
    "            spider1.py   # Your spider implementations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c35849ce-db53-48f1-b820-b6b449c16c3f",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c76a44f1-692e-4da9-86ea-7f2afc53dd72",
   "metadata": {},
   "source": [
    "# 186. Running a Spider\n",
    "Run a spider from the command line:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ea7374c-2550-4079-8fa1-864a02512942",
   "metadata": {},
   "outputs": [],
   "source": [
    "scrapy crawl myspider -o output.json"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c46510a-aa9b-4e13-be2f-b4d3bfd1b42a",
   "metadata": {},
   "source": [
    "Or programmatically:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6272074e-e3f5-4bc8-8bc8-155b6dcdab92",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scrapy.crawler import CrawlerProcess\n",
    "from scrapy.utils.project import get_project_settings\n",
    "\n",
    "process = CrawlerProcess(get_project_settings())\n",
    "process.crawl(MySpider)\n",
    "process.start()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a32c3b8c-fb2e-4b58-b285-4758c3bda4ae",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94453016-a572-440b-85dd-8b0c73bcca14",
   "metadata": {},
   "source": [
    "# 187. Selectors\n",
    "Scrapy provides powerful selectors for extracting data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c97467b-ea88-4d63-81fb-4f6ca57bcb46",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CSS selectors\n",
    "response.css('title::text').get()          # Get first match\n",
    "response.css('p::text').getall()           # Get all matches\n",
    "\n",
    "# XPath selectors\n",
    "response.xpath('//title/text()').get()\n",
    "response.xpath('//p/text()').getall()\n",
    "\n",
    "# Combining selectors\n",
    "response.css('div.product').xpath('.//h2/text()').get()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2971b808-a8ee-4f11-a5bd-4ba24385d141",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24a71f69-16c2-48ee-951b-5aba967ddc20",
   "metadata": {},
   "source": [
    "# 188. Settings\n",
    "Configure Scrapy in settings.py:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "761fe801-bbe0-4e89-ac98-9bd8d45e387d",
   "metadata": {},
   "outputs": [],
   "source": [
    "BOT_NAME = 'myproject'\n",
    "\n",
    "USER_AGENT = 'Mozilla/5.0 (compatible; MyBot/1.0)'\n",
    "\n",
    "ROBOTSTXT_OBEY = True\n",
    "\n",
    "ITEM_PIPELINES = {\n",
    "    'myproject.pipelines.PriceConversionPipeline': 300,\n",
    "    'myproject.pipelines.JsonWriterPipeline': 800,\n",
    "}\n",
    "\n",
    "DOWNLOAD_DELAY = 2  # 2 seconds delay between requests"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5dcecdb5-706d-4939-91cc-b187bef80e9b",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9c00b91-6303-404b-9333-b1eb893b09fb",
   "metadata": {},
   "source": [
    "# 189. Advanced Features"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84c32d56-f1b3-4f4d-8381-b80bfd3d43a8",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a8402fa-654f-4431-be7c-f0f0f6aaad9c",
   "metadata": {},
   "source": [
    "## 189-1. Link Extractors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05ab03d2-bf73-4d94-a4d1-02c817f8007c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scrapy.linkextractors import LinkExtractor\n",
    "\n",
    "class MySpider(scrapy.Spider):\n",
    "    # ...\n",
    "    def parse(self, response):\n",
    "        extractor = LinkExtractor(allow=r'/category/')\n",
    "        for link in extractor.extract_links(response):\n",
    "            yield response.follow(link, callback=self.parse_category)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66851a4e-e878-4f66-a983-046d29d7f74b",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba39139b-037b-43c9-aa24-df7dc541ef1b",
   "metadata": {},
   "source": [
    "## 189-2. Form Handling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ae4055b-eb6f-45da-9176-66bce7bb1c5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LoginSpider(scrapy.Spider):\n",
    "    def start_requests(self):\n",
    "        return [scrapy.FormRequest(\n",
    "            'http://example.com/login',\n",
    "            formdata={'user': 'john', 'pass': 'secret'},\n",
    "            callback=self.after_login\n",
    "        )]\n",
    "\n",
    "    def after_login(self, response):\n",
    "        # Check login success before continuing\n",
    "        if \"authentication failed\" in response.text:\n",
    "            self.logger.error(\"Login failed\")\n",
    "            return\n",
    "        # Continue scraping"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "535258be-5cd3-4e28-852d-d4e539b71fc6",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97aef20a-775f-464b-b30f-56ec6b09b442",
   "metadata": {},
   "source": [
    "## 189-3. Exporting Data\n",
    "Scrapy supports multiple export formats:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed82252c-fe71-4d74-b064-f5b963177a61",
   "metadata": {},
   "outputs": [],
   "source": [
    "scrapy crawl myspider -o items.json\n",
    "scrapy crawl myspider -o items.csv\n",
    "scrapy crawl myspider -o items.xml"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c33b0a6-9e4d-440c-8ce1-ba8819628615",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "011bff6d-62a6-4831-9802-6d7813ee3b5a",
   "metadata": {},
   "source": [
    "## 189-4. Using Feed Exporters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fca3070c-c063-4773-b6ba-9b9a2773ff6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "FEED_FORMAT = 'jsonlines'\n",
    "FEED_URI = 's3://mybucket/%(name)s/%(time)s.json'\n",
    "FEED_EXPORT_FIELDS = ['name', 'price']  # Control field order"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53904bae-daba-4e7c-9cd1-07dfc6a82bf0",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25d8007e-8740-4a76-9a3b-315dc43925e2",
   "metadata": {},
   "source": [
    "# 190. Example Complete Spider\n",
    "Here's a complete e-commerce spider example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e993866c-744d-4809-b94c-14a7388ae09e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import scrapy\n",
    "from myproject.items import ProductItem\n",
    "\n",
    "class EcommerceSpider(scrapy.Spider):\n",
    "    name = 'ecommerce'\n",
    "    allowed_domains = ['example-store.com']\n",
    "    start_urls = ['https://example-store.com/categories']\n",
    "\n",
    "    custom_settings = {\n",
    "        'DOWNLOAD_DELAY': 1,\n",
    "        'CONCURRENT_REQUESTS_PER_DOMAIN': 2,\n",
    "    }\n",
    "\n",
    "    def parse(self, response):\n",
    "        # Extract category links\n",
    "        categories = response.css('div.categories a::attr(href)').getall()\n",
    "        for category in categories:\n",
    "            yield response.follow(category, callback=self.parse_category)\n",
    "\n",
    "    def parse_category(self, response):\n",
    "        # Extract product links\n",
    "        products = response.css('div.product-card a::attr(href)').getall()\n",
    "        for product in products:\n",
    "            yield response.follow(product, callback=self.parse_product)\n",
    "\n",
    "        # Pagination\n",
    "        next_page = response.css('a.next-page::attr(href)').get()\n",
    "        if next_page:\n",
    "            yield response.follow(next_page, callback=self.parse_category)\n",
    "\n",
    "    def parse_product(self, response):\n",
    "        item = ProductItem()\n",
    "        item['name'] = response.css('h1.product-name::text').get().strip()\n",
    "        item['price'] = response.css('span.price::text').get().replace('$', '').strip()\n",
    "        item['description'] = ' '.join(response.css('div.description ::text').getall()).strip()\n",
    "        item['url'] = response.url\n",
    "        \n",
    "        yield item"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d01ea16c-d7a8-4263-9292-948326bebd0c",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eee50cbd-cf5d-4b59-b83d-c92c6fe72d5e",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b40206fb-6731-46dc-9302-0b9bc348b89b",
   "metadata": {},
   "source": [
    "# Some Excercises"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "088d38a8-866c-48a9-a9b5-40df9d99ae4b",
   "metadata": {},
   "source": [
    "**1.** Create a spider that scrapes quotes from http://quotes.toscrape.com:\n",
    "- Extract quote text, author, and tags\n",
    "- Follow pagination links\n",
    "- Save results to quotes.json"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b486b7b8-307c-4eb7-8c14-a1dd72eb2e46",
   "metadata": {},
   "source": [
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c32b073-bab0-451e-8fd7-a294020f71c6",
   "metadata": {},
   "source": [
    "**2.** Modify Exercise 1 to:\n",
    "\n",
    "1. Create a custom Item class for quotes\n",
    "\n",
    "2. Add a pipeline that:\n",
    "    - Converts all tags to lowercase\n",
    "    - Filters out quotes with less than 3 tags\n",
    "    - Stores results in both JSON and CSV"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "489da9fe-be18-487a-bcc7-e5d6672698ba",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf117f39-fa94-453e-bca9-2ccd7b507496",
   "metadata": {},
   "source": [
    "**3.** Scrape http://quotes.toscrape.com/login:\n",
    "\n",
    "- First perform login (username/password can be anything)\n",
    "- After successful login, scrape the protected quotes\n",
    "- Handle login failure cases"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02e59887-780b-4321-9382-ef4c85ec9684",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b32e2d5-3ed7-4865-b63e-21c5cc156f67",
   "metadata": {},
   "source": [
    "**4.** Create a custom middleware that:\n",
    "\n",
    "1. Rotates user agents from a list of 5 different browsers\n",
    "\n",
    "2. Logs all 404 responses to a file\n",
    "\n",
    "3. Retries failed requests with exponential backoff"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8dcb22a4-aff7-4377-b09a-a39fa4a3b2a7",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17511368-bcb2-4413-87ab-450ca9a64ff2",
   "metadata": {},
   "source": [
    "**5.** Scrape books.toscrape.com and:\n",
    "\n",
    "1. Extract book title, price, rating, and image URL\n",
    "\n",
    "2. Use Scrapy's ImagesPipeline to download all book covers\n",
    "\n",
    "3. Save images in folders organized by book rating"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d682728-1f0e-4594-a1d3-30793e3f0751",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25f7d348-8fc5-49e9-a3e0-db0d0374a954",
   "metadata": {},
   "source": [
    "**6.** Create a spider that:\n",
    "\n",
    "1. Uses Scrapy's FormRequest to interact with a mock API (https://httpbin.org/post)\n",
    "\n",
    "2. Sends paginated POST requests with different parameters\n",
    "\n",
    "3. Processes JSON responses and extracts specific data fields"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56db6a03-fcd5-4229-aa81-132e3f4cb731",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3208864f-ffdb-4b05-b0b1-f6b67683a174",
   "metadata": {},
   "source": [
    "**7.** Modify any previous exercise to:\n",
    "\n",
    "1. Use Redis for distributed request queue\n",
    "\n",
    "2. Implement duplicate request filtering\n",
    "\n",
    "3. Run multiple spider instances that coordinate through Redis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31370a8b-cd74-4371-9450-8ce4248412e6",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94bafa50-5d10-40db-97d3-71bdc1821b11",
   "metadata": {},
   "source": [
    "**8.** Create a complete e-commerce scraper for a real site (like Amazon or eBay) that:\n",
    "\n",
    "1. Handles search results pagination\n",
    "\n",
    "2. Extracts product details with error handling\n",
    "\n",
    "3. Uses proxies and proper throttling\n",
    "\n",
    "4. Implements cache for development\n",
    "\n",
    "5. Stores data in MySQL database through pipeline\n",
    "\n",
    "6. Includes monitoring/logging system"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af3414ed-8ba7-485b-954f-69bb05f37262",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cc97f26-cf68-4d50-82a1-ae7dcdfa95ac",
   "metadata": {},
   "source": [
    "#                                                        üåû https://github.com/AI-Planet üåû"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
