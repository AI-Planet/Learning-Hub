{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "355b5f03-d2a5-4aa3-a77c-776ccfcb35d5",
   "metadata": {},
   "source": [
    "# Session 27 üêç"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45efc9ec-fb9c-458c-b1de-d961145e8449",
   "metadata": {},
   "source": [
    "‚òÄÔ∏è‚òÄÔ∏è‚òÄÔ∏è‚òÄÔ∏è‚òÄÔ∏è‚òÄÔ∏è‚òÄÔ∏è‚òÄÔ∏è‚òÄÔ∏è‚òÄÔ∏è‚òÄÔ∏è‚òÄÔ∏è‚òÄÔ∏è‚òÄÔ∏è‚òÄÔ∏è‚òÄÔ∏è‚òÄÔ∏è‚òÄÔ∏è‚òÄÔ∏è‚òÄÔ∏è‚òÄÔ∏è‚òÄÔ∏è‚òÄÔ∏è‚òÄÔ∏è‚òÄÔ∏è‚òÄÔ∏è‚òÄÔ∏è‚òÄÔ∏è‚òÄÔ∏è‚òÄÔ∏è‚òÄÔ∏è‚òÄÔ∏è‚òÄÔ∏è‚òÄÔ∏è‚òÄÔ∏è‚òÄÔ∏è‚òÄÔ∏è‚òÄÔ∏è‚òÄÔ∏è‚òÄÔ∏è‚òÄÔ∏è‚òÄÔ∏è‚òÄÔ∏è‚òÄÔ∏è‚òÄÔ∏è"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c7a6ce5-4cd2-45cd-a3b8-9ac277170654",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f47542a8-416d-4a3f-8c9d-99781dc53bd8",
   "metadata": {},
   "source": [
    "# 214. JAX\n",
    "`JAX` is a high-performance numerical computation library, primarily for scientific computing and machine learning research. It is developed by Google.\n",
    "\n",
    "But that description sells it short. Think of it as NumPy on steroids, fused with a compiler, and designed for the modern hardware landscape (GPUs/TPUs).\n",
    "\n",
    "Its superpower is composable function transformations, which allow you to write elegant, pure Python code that can be automatically differentiated, compiled, and executed in parallel across accelerators."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39132e03-46bb-49e6-950c-4684c3e8bc0d",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cccc2432-7d24-4f29-b7a2-300e65304cd8",
   "metadata": {},
   "source": [
    "# 215. The Foundation: NumPy-like API\n",
    "If you know NumPy, you already know 80% of JAX's basic API. JAX provides a nearly identical interface with `jnp` instead of `np`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17790cdb-b9ee-4fa8-9bc7-e5cb40e982fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import jax.numpy as jnp\n",
    "import numpy as np\n",
    "\n",
    "# They look the same\n",
    "x_np = np.array([1., 2., 3.])\n",
    "x_jnp = jnp.array([1., 2., 3.])\n",
    "\n",
    "# You can do similar operations\n",
    "y_np = np.sin(x_np)\n",
    "y_jnp = jnp.sin(x_jnp)\n",
    "\n",
    "print(\"NumPy: \", y_np)\n",
    "print(\"JAX:   \", y_jnp)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e04a803-5cbd-453c-b62f-ca65e031e661",
   "metadata": {},
   "source": [
    "This familiarity is a huge advantage. You can often port NumPy code to JAX with just a search-and-replace of np to jnp."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a35384c2-ea49-4613-b074-ef3b571f756e",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e4dbbf4-88e4-4a78-b9a0-fd722e1aa1c0",
   "metadata": {},
   "source": [
    "# 216. Immutability\n",
    "JAX arrays are immutable. You cannot do x_jnp[0] = 10. You must create a new array instead. This is fundamental to JAX's functional programming model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0387b542-f48a-4efd-94e8-546d5b426532",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This will ERROR in JAX\n",
    "# x_jnp[0] = 10\n",
    "\n",
    "# Correct JAX way: create a new array\n",
    "new_x_jnp = x_jnp.at[0].set(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4bb5edf-3034-4d79-8edf-8f0740ea0e27",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb5cc52c-9d10-47ad-baeb-0900ab6358ff",
   "metadata": {},
   "source": [
    "# 217. PRNG for Randomness \n",
    "NumPy manages random state implicitly. JAX requires you to handle it explicitly using a PRNG (Pseudo-Random Number Generator) key for reproducibility and parallelism."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c171aa0-494e-4e32-a42f-3644634470c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from jax import random\n",
    "\n",
    "key = random.PRNGKey(42) # Seed of 42\n",
    "key, subkey = random.split(key) # Splitting is common practice\n",
    "random_numbers = random.normal(subkey, shape=(3,))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d2de8aa-11c8-4023-a19b-77faa03c14af",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7528c824-0f13-4c5d-aa16-25b5deb65b5f",
   "metadata": {},
   "source": [
    "# 218. The Magic: Composable Transformations\n",
    "This is the heart of JAX. It provides functions that take a Python function and return a new, transformed function."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96755e49-1296-4c80-b53d-afefe4655d39",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "399eda62-3c8c-4602-bf86-0ed147b3d158",
   "metadata": {},
   "source": [
    "## 218-1. Gradients: grad\n",
    "The grad function performs automatic differentiation. You give it a function that takes arrays and returns a scalar, and it gives you back a new function that computes its gradient."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc311f73-eed9-41a9-941a-989e3728de2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from jax import grad\n",
    "\n",
    "def loss_fn(params, data):\n",
    "    return jnp.sum(params ** 2) # A simple quadratic loss\n",
    "\n",
    "# Create the gradient function\n",
    "grad_fn = grad(loss_fn)\n",
    "\n",
    "params = jnp.array([1.0, 2.0, 3.0])\n",
    "data = jnp.ones(3) # Dummy data\n",
    "\n",
    "gradients = grad_fn(params, data)\n",
    "print(gradients) # [2. 4. 6.] (the gradient of sum(x_i^2) is 2*x_i)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89d79214-4d24-4f11-bff3-2a5c092c85e7",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d78187cc-be37-4780-af75-09adb0339955",
   "metadata": {},
   "source": [
    "## 218-2. Just-In-Time Compilation: jit\n",
    "The jit function compiles your function using XLA (Accelerated Linear Algebra), the same compiler that powers TensorFlow and Google's TPUs.\n",
    "\n",
    "Python is slow. XLA analyzes your function's operations, fuses them together, and generates highly optimized machine code for CPUs, GPUs, or TPUs. This leads to massive speedups, especially on accelerators."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49e05f1b-864d-4878-8daf-c7ca191680b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from jax import jit\n",
    "\n",
    "def slow_fn(x):\n",
    "    return x * x + x * 2.0 # Some element-wise operation\n",
    "\n",
    "# Compile it!\n",
    "fast_fn = jit(slow_fn)\n",
    "\n",
    "# First call has compilation overhead, subsequent calls are blazing fast\n",
    "result = fast_fn(jnp.ones((1000, 1000)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5d2ee3a-efa5-4cc4-a34a-77e00ce9a9a4",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6054fd00-b5cb-4760-84ea-8f2798c54d08",
   "metadata": {},
   "source": [
    "## 218-3. Vectorization / Automatic Parallelization: vmap\n",
    "The vmap function automatically adds a batch dimension to your function. It \"vectorizes\" a function written for a single example so it can efficiently process a batch of examples.\n",
    "\n",
    "It eliminates the need to write tedious batch dimensions and loops, reduces code complexity, and often allows jit to generate even more efficient code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a6f6ca1-050c-4a51-aa7b-f527f40bcc9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from jax import vmap\n",
    "\n",
    "# A function that works on a single vector\n",
    "def predict_single(params, input_vec):\n",
    "    return jnp.dot(params, input_vec)\n",
    "\n",
    "# Let's say we have a batch of 100 input vectors\n",
    "batched_input = random.normal(key, (100, 5))\n",
    "params = random.normal(key, (5,))\n",
    "\n",
    "# Naive way: slow Python loop\n",
    "predictions_loop = jnp.array([predict_single(params, x) for x in batched_input])\n",
    "\n",
    "# JAX way: automatic vectorization\n",
    "predict_batched = vmap(predict_single, in_axes=(None, 0)) # Don't batch params (None), batch the 0th axis of input\n",
    "predictions_vmap = predict_batched(params, batched_input)\n",
    "\n",
    "# predictions_loop and predictions_vmap will be identical, but vmap is much faster and jit-able!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c35849ce-db53-48f1-b820-b6b449c16c3f",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c76a44f1-692e-4da9-86ea-7f2afc53dd72",
   "metadata": {},
   "source": [
    "## 218-4. Parallelization across devices: `pmap`\n",
    "The `pmap` function is like `vmap`, but instead of vectorizing along a batch dimension, it parallelizes it across multiple cores/devices (e.g., multiple GPUs). It handles all the communication (e.g., gradient synchronization) required for data-parallel training."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a32c3b8c-fb2e-4b58-b285-4758c3bda4ae",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94453016-a572-440b-85dd-8b0c73bcca14",
   "metadata": {},
   "source": [
    "# 219. The Power of Composition\n",
    "The true genius of JAX is that these transformations are composable. You can `jit` a `grad` of a `vmap`'d function. This composability is what makes it so powerful and expressive for research."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c97467b-ea88-4d63-81fb-4f6ca57bcb46",
   "metadata": {},
   "outputs": [],
   "source": [
    "# A more complex example: Compute the per-example gradient for a whole batch, then compile it.\n",
    "per_example_grad = jit(vmap(grad(loss_fn), in_axes=(None, 0)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2971b808-a8ee-4f11-a5bd-4ba24385d141",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24a71f69-16c2-48ee-951b-5aba967ddc20",
   "metadata": {},
   "source": [
    "# 220. JAX's Ecosystem: Libraries Built on JAX\n",
    "You rarely use raw JAX directly for complex models. Instead, you use powerful libraries that sit on top of it and handle neural network specifics:\n",
    "\n",
    "`Flax:` A high-level, flexible neural network library developed by Google. It's the most popular choice for new projects, providing modules, optimizers, and serialization. (The \"Keras/TF\" for JAX).\n",
    "\n",
    "`Haiku:` A neural network library from DeepMind. It's more object-oriented and closer to the Sonnet style (from TensorFlow). It emphasizes a clean separation between model definition and state.\n",
    "\n",
    "`Optax:` A standard library for gradient processing and optimization (like Adam, SGD). It's used by both Flax and Haiku.\n",
    "\n",
    "`RLax:` A library for reinforcement learning building blocks."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5dcecdb5-706d-4939-91cc-b187bef80e9b",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eee50cbd-cf5d-4b59-b83d-c92c6fe72d5e",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b40206fb-6731-46dc-9302-0b9bc348b89b",
   "metadata": {},
   "source": [
    "# Some Excercises"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "088d38a8-866c-48a9-a9b5-40df9d99ae4b",
   "metadata": {},
   "source": [
    "**1.** Create a 1D JAX array x with the values [5, 10, 15, 20].\n",
    "\n",
    "Try to change the third element (value 15) to a 99 using standard Python assignment (x[2] = 99).\n",
    "\n",
    "Observe and understand the error.\n",
    "\n",
    "Now, do it correctly using the .at[].set() method and store the result in a new variable x_new.\n",
    "\n",
    "Print both x and x_new to confirm the original array was unchanged."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b486b7b8-307c-4eb7-8c14-a1dd72eb2e46",
   "metadata": {},
   "source": [
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c32b073-bab0-451e-8fd7-a294020f71c6",
   "metadata": {},
   "source": [
    "**2.** Create a PRNG key with a seed of 0.\n",
    "\n",
    "Use this key to generate a random array of shape (2, 3) from a normal distribution (mean=0, std=1). Store it as random_array_1.\n",
    "\n",
    "Now, generate another random array of the same shape from the same distribution using the same original key. Store it as random_array_2.\n",
    "\n",
    "Check if random_array_1 and random_array_2 are identical. They should be.\n",
    "\n",
    "Now, split the original key to get a new subkey. Use this subkey to generate a new random array, random_array_3.\n",
    "\n",
    "Verify that random_array_1 and random_array_3 are different."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "489da9fe-be18-487a-bcc7-e5d6672698ba",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf117f39-fa94-453e-bca9-2ccd7b507496",
   "metadata": {},
   "source": [
    "**3.** Define a simple function f(x) that calculates the sum of the cubic function: $f(x) = \\sum(x^3)$.\n",
    "\n",
    "Define a point x = jnp.array([1.0, 2.0, 3.0, 4.0]).\n",
    "\n",
    "Manually compute what the gradient of f(x) should be at point x (hint: $\\frac{d}{dx}x^3 = 3x^2$).\n",
    "\n",
    "Now, use jax.grad to create a gradient function df_dx = grad(f).\n",
    "\n",
    "Call df_dx(x) and verify the result matches your manual calculation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02e59887-780b-4321-9382-ef4c85ec9684",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b32e2d5-3ed7-4865-b63e-21c5cc156f67",
   "metadata": {},
   "source": [
    "**4.** Create a large random vector v of size 10,000,000.\n",
    "\n",
    "Write a function slow_function(x) that performs a non-trivial element-wise operation (e.g., x * x + jnp.sin(x) * jnp.cos(x) ** 2).\n",
    "\n",
    "Time how long it takes to execute result = slow_function(v).\n",
    "\n",
    "Now, create a JIT-compiled version: fast_function = jit(slow_function).\n",
    "\n",
    "Time the first call to fast_function(v) (this includes compilation time).\n",
    "\n",
    "Time a second call to fast_function(v). Observe the massive speedup on the compiled call."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8dcb22a4-aff7-4377-b09a-a39fa4a3b2a7",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17511368-bcb2-4413-87ab-450ca9a64ff2",
   "metadata": {},
   "source": [
    "**5.** Define a function predict that takes weights and a single input vector, and returns a single prediction (e.g., a dot product plus a bias: result = jnp.dot(weights, input) + bias).\n",
    "\n",
    "Create a batch of 1000 input vectors, batch_inputs, of shape (1000, 5).\n",
    "\n",
    "Create weights and bias.\n",
    "\n",
    "Compute the predictions for the entire batch using a naive Python for loop. Time it.\n",
    "\n",
    "Now, use vmap to create a batched version of predict. Hint: You'll need to specify which axes to map over (e.g., in_axes=(None, 0) for (weights, input)).\n",
    "\n",
    "Compute the predictions on the entire batch using the vmap'd function. Time it and compare the speed and result to the loop method."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d682728-1f0e-4594-a1d3-30793e3f0751",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25f7d348-8fc5-49e9-a3e0-db0d0374a954",
   "metadata": {},
   "source": [
    "**6.** Define a function f(x) that takes a 3-element vector and returns a 2-element vector (e.g., f(x) = jnp.array([x[0] * x[1], x[1] ** 2 + x[2]])).\n",
    "\n",
    "Define an input point x = jnp.array([1.0, 2.0, 3.0]).\n",
    "\n",
    "The Jacobian is a matrix $J$ where $J_{ij} = \\frac{\\partial f_i}{\\partial x_j}$.\n",
    "\n",
    "Use a combination of vmap and grad to compute the Jacobian of f at point x in a single, elegant line of code without any loops.\n",
    "Hint: vmap(grad(f)) gives you the gradient of the first output."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56db6a03-fcd5-4229-aa81-132e3f4cb731",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3208864f-ffdb-4b05-b0b1-f6b67683a174",
   "metadata": {},
   "source": [
    "**7.** Define a simple mean-squared-error loss function mse(params, input, target) for a single data point. Assume params is a dictionary containing {'weights': w, 'bias': b}.\n",
    "\n",
    "Your goal is to create a function that calculates the gradient of the loss with respect to the params for an entire batch of inputs and targets, and does it as fast as possible.\n",
    "\n",
    "Construct this function by composing:\n",
    "- a) grad to get the gradient for a single example.\n",
    "- b) vmap to vectorize this gradient function over a batch.\n",
    "- c) jit to compile the entire thing for maximum speed.\n",
    "\n",
    "Apply this function to a dummy batch of data and print the resulting gradients."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31370a8b-cd74-4371-9450-8ce4248412e6",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94bafa50-5d10-40db-97d3-71bdc1821b11",
   "metadata": {},
   "source": [
    "**8.** Install Flax: pip install flax\n",
    "\n",
    "Import flax.linen as nn.\n",
    "\n",
    "Define a simple multi-layer perceptron (MLP) class MLP(nn.Module): with one hidden layer of 64 units and a output layer of 10 units (e.g., for classification). Use @nn.compact and the nn.Dense module.\n",
    "\n",
    "Initialize a PRNG key.\n",
    "\n",
    "Create a dummy input batch of shape (32, 784) (e.g., mimicking 32 MNIST images).\n",
    "\n",
    "Use the init method to initialize the parameters of the model.\n",
    "\n",
    "Use the apply method to perform a forward pass of the dummy batch through the network."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af3414ed-8ba7-485b-954f-69bb05f37262",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cc97f26-cf68-4d50-82a1-ae7dcdfa95ac",
   "metadata": {},
   "source": [
    "#                                                        üåû https://github.com/AI-Planet üåû"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
